% !TeX root = ./paper.tex
\documentclass[modern]{aastex62}

% Load the corTeX style definitions
\input{cortex}

% Bibliography stuff
\bibliographystyle{aasjournal}

% Begin!
\begin{document}

% Title
%\title{Inferring a time dependent map of Io's surface from occultations and phase curves.}
\title{Occultation mapping of Io's surface in the near-infrared II: Inferring time variable maps using latent Nonnegative Matrix Factorization}

% Author list
\author{Fran Bartoli\'c}
\email{fbartolic@flatironinstitute.org}
\affil{Center~for~Computational~Astrophysics, Flatiron~Institute, New~York, NY}
\affil{Centre for Exoplanet Science, SUPA, School of Physics and Astronomy, University of St. Andrews, St. Andrews, UK}
\author{Rodrigo Luger}
\author{Daniel Foreman-Mackey}
\affil{Center~for~Computational~Astrophysics, Flatiron~Institute, New~York, NY}
%

\begin{abstract}
    
\end{abstract}

%
\section{Introduction}
\section{Data}
\label{sec:data}

\begin{figure}[h!]
    \begin{centering}
    \includegraphics[width=0.5\linewidth]{figures/irtf_max_flux.pdf}
    \oscaption{irtf_dataset}{%
        Maximum flux for each curve taken using NASA's IRTF telescope during an
        occultation of Io by Jupiter.
       \label{fig:irtf_max_flux}
    }
    \end{centering}
\end{figure}

The dataset we use to infer maps of Io's surface consists of 112 observations of Io in the near infrared taken from 1996 until 2018 using various instruments at NASA's IRTF observatory.
Figure~\ref{fig:irtf_max_flux} shows the maximum flux in $\mathrm{GW}/\mu \mathrm{m}/\mathrm{sr}$ for each light curve plotted as a function of time which approximately corresponds to the disc integrated flux of Io at the beginning of ingress or the end of egress prior to it being occulted by Jupiter.
Two features of this plot are apparent.
First although the observations span decades the cadence is non-uniform with only a few observations taken between 2008 and 2016.
Second, the baseline brightness is varying stochastically by a large amount and there are several notable events of increased volcanic activity.
All of the observations were taken while Io was \emph{in eclipse}, meaning that it was in Jupiter's shadow and all of the observed emission from the surface is due to thermal radiation.
Observations of Io \emph{in sunlight} on the other hand probe both the thermal (volcanic) emission and the surface albedo variations in the near infrared.


\begin{figure}[h!]
    \begin{centering}
    \includegraphics[width=0.5\linewidth]{figures/irtf_sample_lightcurves.pdf}
    \oscaption{irtf_dataset}{%
        A selection of sample light curves taken during occultations of Io by Jupiter in our dataset.
        The step-like morphology of the light curves is due to bright volcanoes on Io's surface coming in or out of view during an occultation.
        These light curves thus visibly encode information about the features on the surface.
        \label{fig:irtf_sample_lightcurves}
    }
    \end{centering}
\end{figure}

In Figure~\ref{fig:irtf_sample_lightcurves} we display a random subset of all the light curves
in the dataset.
All occultations last for $\sim4$ min and the cadence for each light varies but it on the order of a second.
It follows that over the course of a single exposure Jupiter's limb move by about 15km which provides a lower bound for the size of the features on the surface that we can reliably estimate.
The shapes of light curves strongly deviate from the smooth variability one would expect assuming a homogeneous distribution of thermal emission on the surface.
Especially prominent are light curves with clear step-like features which are present when bright spots come in or out of view during the course of an occultation.
The fact that these features are so clearly visible means that even individual light curves encode a wealth of information about the surface.

The photometric quality varies from year to year because multiple instruments were used over the years.
An additional issue is that estimated errorbars aren't provided.
As with all ground based photometry, the observations are influenced by atmospheric variability which induces correlated noise in the light curves.
Because of this the flux isn't always monotonically increasing or decreasing as one would expect.
All of these issues need to be accounted for in the final model.

\section{Nonnegative Matrix Factorization}
\label{sec:nmf}
In the case when we cannot assume a static map, the situation is considerably more complicated.
In principle, the surface map is different for each \emph{data point}.
Of course, fitting one map per data point is intractable so we might instead fit a single map per light curve, generated by Eq.~\ref{eq:linear_model_pix}.
Even then, we would need to fit on the order of 1k parameters!
This is both becuse the number of spherical coefficients scales as $(l+1)^2$ and because when fitting in pixel space we need many more pixels than spherical harmonics to ensure positivity.
Although fitting a model of such high dimensionality for a single IRTF light curve of high signal to noise ratio is tractable thanks to exact gradients provided by autodifferentiation in \textsf{starry}, we don't want to do that for a hundred or so separate light curves.
Such a model would be very difficult to fit and it would require strong regularization between successive maps in time.
It also wouldn't directly provide much physical insight into the volcanic activity, we would have to conduct a separate analysis on the inferred maps to constrain the time variability of the volcanoes.
Instead, we need an approach which reduces the dimensionality of the problem.
One way of accomplishing that is to expand the spherical harmonic coefficients (or pixels) in a Taylor series in time about a certain point.
This is the approach \cite{luger_tess_2019} took to model a map of Earth's albedo in reflected light from stray Earthshine in the aperture of the TESS space telescope.
This issue with this approach and similar series expansions such as the Fourier series is that with the very long baseline of IRTF data and the observations being scattered throughout the years sporadically, it is unclear which point should be the origin of the expansion.
More importantly, there likely doesn't exist a well defined global time-averaged map of Io so requiring too much smoothness between successive maps wouldn't work.
Another way of reducing the dimensionality of the problem would be a parametric approach where where we place a few spots on the sphere and then fit only for the locations and intensities of each spot.
This is assuming that we know a priori that the surface features are spot like and roughly how many we should expect.
While such a strong assumption might work for studying individual spots in the context of Io, it certainly isn't justified for exoplanets.
In Section~\ref{sec:inverse_problem} we do assume stronger priors on the locations of and shapes of surface features to better constrain the variability of individual volcanoes, but we still fit for pixels, at least in principle allowing the data to override our assumptions.

The approach approach opted for is to treat the problem of inferring a single map per light curve as a probabilistic matrix factorization problem.
The idea is to assume that a map for any given light curve can be expressed as a linear combination of $K$ "basis maps".
Consider $L$ lightcurves, the model for the $l$-th light curve is
\begin{equation}
    \mathbf{f}_l=\mathbf{A}_l\,\mathbf{P}^\dagger\,\mathbf{p}_l'
\end{equation}
We can stack the column vectors $\mathbf{p}_l$ into a matrix $\mathbf{Y}$ of shape $(N_p, L)$ where $N_p$ is the number of pixels for each light curve.
We then assume that the $\mathbf{Y}$ can be decomposed into a product of two matrices $\mathbf{B}$ and $\mathbf{Q}$ as
\begin{equation}
    \mathbb{Y}=\mathbf{B}\,\mathbf{Q}
    \label{eq:nmf}
\end{equation}
where $\mathbf{B}$ has shape $(N_p, K)$ and $\mathbf{Q}$ has shape $(K, L)$.
The model for all light curves can be written as
\begin{equation}
    \mathbb{f}=\mathbb{A}\,\mathrm{vec}(\mathbf{\mathbb{Y}})
    \label{eq:model_all_lcs}
\end{equation}
Where $\mathbf{f}$ is a tall column vector consisting of predictions for all light curves stacked together, $\mathbb{A}$ is a block diagonal matrix with matrices $\mathbf{A}_l\,\mathbf{P}^\dagger$ on the diagonal for $l=1\dots, L$ and the $\mathrm{vec}$ operator stacks the columns of $\mathbb{Y}$ into a tall vector.
The model is \emph{bilinear} which means that is linear in the matrices $\mathbf{B}$ and $\mathbf{Q}$ separately.
The interpretation of Equation~\ref{eq:nmf} is simple, each of the $K$ columns of $\mathbf{B}$ represents pixels of a basis map and the columns of $\mathbf{Q}$ determine how those maps add together to produce a final map for the $l$-th light curve.
For this reason we call $\mathbf{B}$ the \emph{basis matrix} and the matrix $\mathbf{Q}$ the \emph{encoding matrix}.
Ideally, we want the basis maps to be physically meaningful and the coefficients encode the time variability.
A nice feature of this model is that there is no requirement that successive maps are smoothly varying between different light curves.

The matrix factorization problem as written in Equation~\ref{eq:nmf} is highly degenerate and practically intractable a probabilistic framework with a small dataset.
Since each of the columns of $\mathbf{B}$ are pixels representing emitted light or albedo, we can substantially reduce the ambiguity in the decomposition by requiring that both matrices are strictly positive.
This matrix factorization problem is known as \emph{Nonnegative Matrix Factorization} (NMF) \citep{paatero_positive_1994,lee_algorithms_2001} and it has a rich history across many different fields.
It is commonly used for decomposing physical signals in the spectral or the time domain.
Even though NMF is more tractable than unconstrained matrix factorization, it is still an NP hard problem \citep{vavasis_complexity_2009} and it requires additional constraints or priors to be tractable.
Simultaneously transforming $\mathbf{B}\leftarrow \mathbf{B}\,\mathbf{S}^{-1}$ and
$\mathbf{Q}\leftarrow \,\mathbf{S}\mathbf{Q}$ with a nonsingular matrix $\mathbf{S}$ doesn't change the value of the objective function.
The objective function is also invariant to a simultaneous permutation of the columns of $\mathbf{B}$ and rows of $\mathbf{Q}$ and rescaling either of the two matrices by a scalar.
Nevertheless, getting around these degeneracies in practice is possible, especially when we have a lot of prior knowledge on the problem.
For a recent review article with a detalied analysis of these degeneracies and common algorithms, see \cite{fu_nonnegative_2019}.


Broadly, there are two approaches to fitting NMF models, the constrained optimization approach and the probabilistic approach.
In the optimization approach we optimize $\mathbb{Y}$ in a least-squares sense under a set of constraints on either one or both of the matrices \citep[see][for recent examples from the astronomical literature]{acosta-pulido_new_2017, melchior_scarlet_2018,ren_non-negative_2018,ren_using_2020} with the goal of obtaining a point estimate for the two matrices.
The probabilistic approach introduces priors and the inference is usually done with variational inference in order to obtain an approximate posterior distribution over the matrices.
In this work we opt for the probabilistic approach because we care about the uncertainties on the map features a lot.

One difference between most applications of NMF in the literature and NMF within the context of inferring surface maps from 1D light curves is that
in the majority of the work in the literature \citep[except][]{kawahara_global_2020} the matrix $\mathbb{Y}$ is assumed to be directly observed  up to a simple noise term.
In our case the problem is considerably more challenging because we observe the light curves
$\mathbb{A}\,\mathrm{vec}(\mathbf{\mathbb{Y}})$ instead of $\mathbb{Y}$ directly, a process in which some information is lost because, depending on the geometry of the occultations, certain combinations of spherical harmonic coefficients will be in the nullspace.
\cite{kawahara_global_2020} are using NMF to model phase variations in directly imaged exoplanets with the goal of simultaneously inferring a surface map and a spectral decomposition of the map into several components, a case in which the
matrix which is to be decomposed is also unobserved.
Although we don't attempt to solve the problem of spectrally decomposing the maps, our approach is directly applicable to that problem as well. (TODO: should I add more details here?)
One could also imagine inferring a multi spectral component and time dependent map by solving two NMF problems simultaneously.
We leave this application for future work.


\subsection{The likelihood}
\label{ssec:likelihood}
Given Eq.~\ref{eq:nmf} we can write down the likelihood for our data and in Section~\ref{sec:inverse_problem} we dicuss in detail the priors which make the problems of inferring static and dynamic maps tractable.
Assuming a Gaussian noise process for the data with a dense covariance matrix, the (log) likelihood is
\begin{equation}
    \ln\mathcal{L}=-\frac{1}{2}\left[\mathbb{f}_\mathrm{obs}-(\mathbb{f} + \mathbb{b})\right]^{\top} \boldsymbol{\Sigma}^{-1}\left[\mathbb{f}_\mathrm{obs}-(\mathbb{f} + \mathbb{b})\right]
\end{equation}
where $\mathbb{f}_\mathrm{obs}$ is the vector of stacked observed light curves, $\boldsymbol{\Sigma}$ is the data covariance matrix and $\mathbb{b}$ is a fixed flux offset per light curve accounting for stray flux not attributed to Io (mostly due to Jupiter).
We model the data covariance with a Gaussian Process using the fast Celerite method\citep{foreman-mackey_fast_2017} as implemented in the \textsf{exoplanet} package (TODO: cite) to capute correlations imposed by the seeing plus an additional white noise term which is different for each light curve.
The IRTF data isn't provided with any uncertainties so we estimate the uncertainties by filtering the data with the Savitzky Golay filter as implemented in \textsf{SciPy} (TODO: CITE) and estimating the variance of the residuals.
We assume identical errors for all data points in a given light curve but allow for a rescaling factor when fitting the model.
In the next section, we add priors to this likelihood and fit the model on simulated data using optimization and Variational Inference (VI).

\section{The inverse problem}
\label{sec:inverse_problem}

\subsection{Fitting a dynamic map}
\label{ssec:dynamic_map}
% Priors for NMF. Optimization vs. bayesian approach to NMF. Results on simulated data. Comparison of results to matrix factorization without the positivity constraint and clever priors.

\section{Results}
\label{sec:results}

\subsection{Individual events}
\label{ssec:individual}
% Fits of individual events. In particular, those light curves that were analyzed in previous papers.

\subsection{The time-variable map}
\label{ssec:time_variable_map}
%

\subsection{Variability of known hotspots}
\label{ssec:variability_hotspots}
% Plot inferred intensities of known hotspots as function of time, reference previous work.

\section{Mapping volcanic exoplanets}
\label{sec:exoplanets}
% Application to exoplanets. Fitting mock JWST observations.

\section{Conclusions}
\label{sec:conclusions}

% Bibliography
\bibliography{bib}
\end{document}
